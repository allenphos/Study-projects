{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    raw_df: pd.DataFrame, test_size: float = 0.2, val_size: float = 0.25, random_state: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits the raw dataset into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): The raw dataframe.\n",
    "        test_size (float): Proportion of data to be used as the test set.\n",
    "        val_size (float): Proportion of training data to be used as validation.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: train_df, val_df, test_df.\n",
    "    \"\"\"\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        raw_df, test_size=test_size, stratify=raw_df['Exited'], random_state=random_state\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_size, stratify=train_val_df['Exited'], random_state=random_state\n",
    "    )\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def select_features_and_target(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
    "    \"\"\"\n",
    "    Selects the feature columns and target column from the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series, List[str]]: Inputs (features), target variable, list of input column names.\n",
    "    \"\"\"\n",
    "    input_cols = list(df.columns)[3:-1]  # Exclude first 3 columns and target column\n",
    "    target_col = 'Exited'\n",
    "    return df[input_cols], df[target_col], input_cols\n",
    "\n",
    "\n",
    "def identify_column_types(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Identifies numeric and categorical columns in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: List of numeric columns and list of categorical columns.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "def create_preprocessor(numeric_cols: List[str], categorical_cols: List[str], scale_numeric: bool = True) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Creates a preprocessing pipeline for numerical and categorical features.\n",
    "\n",
    "    Args:\n",
    "        numeric_cols (List[str]): List of numerical feature names.\n",
    "        categorical_cols (List[str]): List of categorical feature names.\n",
    "        scale_numeric (bool): Whether to apply scaling to numerical features.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: Preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())]) if scale_numeric else 'passthrough'\n",
    "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def preprocess_data(raw_df: pd.DataFrame, scale_numeric: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Preprocesses the raw dataframe.\n",
    "\n",
    "    Args:\n",
    "        raw_df (pd.DataFrame): The raw dataframe.\n",
    "        scale_numeric (bool): Whether to scale numeric features.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing processed inputs and targets for train, val, and test sets.\n",
    "    \"\"\"\n",
    "    train_df, val_df, test_df = split_data(raw_df)\n",
    "\n",
    "    train_inputs, train_targets, input_cols = select_features_and_target(train_df)\n",
    "    val_inputs, val_targets, _ = select_features_and_target(val_df)\n",
    "\n",
    "    numeric_cols, categorical_cols = identify_column_types(train_inputs)\n",
    "\n",
    "    preprocessor = create_preprocessor(numeric_cols, categorical_cols, scale_numeric)\n",
    "\n",
    "    return {\n",
    "        'X_train': train_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'X_val': val_inputs,\n",
    "        'val_targets': val_targets,\n",
    "        'input_cols': input_cols,\n",
    "        'scaler': preprocessor.transformers[0][1] if scale_numeric else None,\n",
    "        'encoder': preprocessor.transformers[1][1]\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_new_data(new_data: pd.DataFrame, input_cols: List[str], scaler: Any, encoder: OneHotEncoder) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses new data using the provided scaler and encoder.\n",
    "\n",
    "    Args:\n",
    "        new_data (pd.DataFrame): The new dataset to be processed.\n",
    "        input_cols (List[str]): List of input feature names used in training.\n",
    "        scaler (Any): Scaler used for numerical data (if applicable).\n",
    "        encoder (OneHotEncoder): Encoder used for categorical data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed new data.\n",
    "    \"\"\"\n",
    "    new_inputs = new_data[input_cols]\n",
    "\n",
    "    numeric_cols, categorical_cols = identify_column_types(new_inputs)\n",
    "\n",
    "    if scaler:\n",
    "        new_inputs[numeric_cols] = scaler.transform(new_inputs[numeric_cols])\n",
    "\n",
    "    encoded_cats = encoder.transform(new_inputs[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    return pd.concat([new_inputs[numeric_cols].reset_index(drop=True), encoded_df], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
